{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "    div:has(> hr) h1 {\n",
    "        font-size: 5em !important;\n",
    "    }\n",
    "    .reveal h3 {\n",
    "        margin-top: 1em;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# Model Deployment\n",
    "\n",
    "*Model formats, batch vs realtime scoring, and deployment pipelines*\n",
    "\n",
    "---\n",
    "\n",
    "Ethan Swan, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Topic: Deployment\n",
    "\n",
    "Once a model has been selected, we usually need to do something with it.\n",
    "\n",
    "*e.g.* We want to predict what products we should suggest to an online user. \n",
    "\n",
    "Setting up a model to run automatically on new data is called **deployment**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. About Me\n",
    "2. Exporting a Model\n",
    "3. Batch, Realtime, & On-demand Scoring\n",
    "4. Deployment Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- Senior Backend Developer at [ReviewTrackers](https://www.reviewtrackers.com/)\n",
    "    - Startup, ~100 employees\n",
    "    - SaaS platform for online reputation management\n",
    "- Analytics & ML Engineering Team\n",
    "- NLP Microservice (Python), Main API Layer (Go)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Started in February 2022\n",
    "- Wanted to see a techy startup from the inside\n",
    "    - especially engineering practices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- Previously: [84.51˚](https://www.8451.com/)\n",
    "    - Marketing Analytics Branch of Kroger\n",
    "    - Lead Data Scientist - Internal Tools & Infrastructure\n",
    "- Education: University of Notre Dame\n",
    "    - B.S. in Computer Science\n",
    "    - M.B.A."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- 8451:\n",
    "    - Did some measurement work\n",
    "    - quickly transitioned to functional support\n",
    "    - taught classses and helped with tech strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Model Deployment?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Do We Deploy Models?\n",
    "\n",
    "- A model is ultimately a function that maps inputs (*features*) to outputs (*targets*).\n",
    "    - Usually Python or R code\n",
    "- How can we use that function in a real-world application?\n",
    "    - How do we get new data into the model?\n",
    "    - What happens if we shut down the session? Is the model gone forever?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deploying the Model\n",
    "\n",
    "1. Export (save) the model in a reusable place and format.\n",
    "2. Build batch, streaming, or on-demand scoring system that loads the model and uses it to score new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bonus: whenever you create a new model, you can just drop it into the deployment system and overwrite the old one.\n",
    "\n",
    "*(though modern ML Ops best-practice is to tag your models with versions and save old ones.)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models: Formats\n",
    "\n",
    "- Pickle\n",
    "    - Special, non--human-readable binary format\n",
    "    - Can save any Python object\n",
    "    - Some compatibility issues\n",
    "- Raw weights/parameters\n",
    "    - Just a bunch of numbers in a file\n",
    "    - More common for TensorFlow, PyTorch, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Pickle and similar libraries are easier and **more flexible**\n",
    "    - but compatibility concerns\n",
    "- raw model weights are **more portable**\n",
    "    - but not necessarily easy to reload\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models: Locations\n",
    "\n",
    "\n",
    "- Local filesystem\n",
    "    - Only if the model is going to be deployed locally\n",
    "- Cloud storage\n",
    "    - S3, GCS, Azure Blob Storage, etc.\n",
    "    - Works for almost any deployment location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment Approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch, Streaming, and On-demand Scoring\n",
    "- **Batch**\n",
    "    - Run the model in advance and save the output\n",
    "    - Think: Spotify Discover Weekly\n",
    "- Realtime\n",
    "    - Run the model on new data as it comes in\n",
    "    - **Streaming**\n",
    "        - Queue up data and run the model \n",
    "        - Think: Facebook makes new friend recommendations soon after you add a friend\n",
    "    - **On-demand**\n",
    "        - Run the model only when the prediction is needed via an API\n",
    "        - Think: GitHub Copilot recommmends code as you type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Scoring\n",
    "\n",
    "A system runs the model on a batch of data every hour, day, week, etc., and saves the output to a database to be used when needed.\n",
    "\n",
    "### Architecture\n",
    "- Airflow or cloud-based scheduler to kick off the model\n",
    "- Chain parts of the job (tasks) together\n",
    "- Save output to a persistent location: database or cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Scoring\n",
    "\n",
    "![Batch Scoring Architecture](images/batch_architecture.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Scoring\n",
    "\n",
    "Pros\n",
    "- Predictable workload (always one run per hour/day/week)\n",
    "- Relatively easy to set up\n",
    "\n",
    "Cons\n",
    "- Predictions are stale until the next run\n",
    "- Reruns happen even if nothing has changed -- wasting resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Realtime Scoring\n",
    "\n",
    "- **Streaming** – Trigger the model on new \"events\"\n",
    "- **On-demand** – Access the model via an API when new predictions are needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Streaming\n",
    "\n",
    "Track new events and send them into a queue to process. This is **realtime-ish**.\n",
    "- Typically on a delay, which can be large if the queue is long\n",
    "\n",
    "\n",
    "### Architecture\n",
    "- A \"publisher\" sends a message to a \"subscriber\" when an event occurs\n",
    "- Messages are \"queued\" up until the subscriber is ready to process them\n",
    "    - Thus not truly realtime\n",
    "- Platform: Kafka, RabbitMQ, cloud-based pub/sub, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Streaming\n",
    "\n",
    "![Streaming Architecture](images/streaming_architecture.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Streaming\n",
    "\n",
    "Pros\n",
    "- Queues help manage spikes in workload\n",
    "- Consumers (scorers) can run in parallel\n",
    "    - Enables easy horizontal scaling\n",
    "- Queues add additional resilience\n",
    "    - A consumer crash doesn't result in lost data\n",
    "\n",
    "Cons\n",
    "- Requires an additional system (the message queue service)\n",
    "- Data flow through queues is difficult to trace and reason about\n",
    "- Large workloads can cause long backups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On-demand\n",
    "\n",
    "Build an API that accepts new data and returns scores immediately\n",
    "- Typically response time of under a second\n",
    "\n",
    "### Architecture\n",
    "- Write API server in a language that can run the model\n",
    "    - If Python, use Flask or FastAPI framework\n",
    "    - Alternatively, sometimes people rebuild the model in a different language for performance (not recommended)\n",
    "- Deploy the API server to a cloud service that supports scalability\n",
    "- Each instance loads a copy of the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On-demand\n",
    "\n",
    "![On-demand Architecture](images/on_demand_architecture.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On-demand\n",
    "\n",
    "Pros\n",
    "- Only runs the model when needed\n",
    "- No queueing system that can back up\n",
    "- Easy to parallelize -- just run many instances of the API\n",
    "\n",
    "Cons\n",
    "- Slow scoring process can cause problems\n",
    "    - APIs responding slowly can cause timeouts\n",
    "- System can be overwhelmed by a large number of requests\n",
    "    - Need to rate limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to Choose?\n",
    "\n",
    "- Start with batch if you can: easiest to set up\n",
    "    - No need for a queueing system or complex API deployment\n",
    "    - Latency won't be an issue -- scores are just stored in a database to be retrieved any time\n",
    "- If you need realtime...\n",
    "    - Use on-demand if predictions must be served immediately\n",
    "        - If users are waiting for the prediction, you need to get it to them\n",
    "    - If delay is tolerable, or if system overload is a real risk, use streaming and queues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment Pipelines\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment Pipelines\n",
    "\n",
    "- When deployed, a model goes from the domain of *data science* to the domain of *software engineering*, and that means...\n",
    "- Version control\n",
    "    - Can be on the model itself or just a reference to the name/version of the model\n",
    "- CI/CD\n",
    "    - Continuous Integration - automated testing\n",
    "    - Continuous Deployment - automated deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment Pipelines\n",
    "\n",
    "- What's important:\n",
    "    - The model can't get to production without being tested\n",
    "    - It's possible to \"roll back\" to a past version of the model if issues are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
