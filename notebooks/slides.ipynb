{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "    div:has(> hr) h1 {\n",
    "        font-size: 5em !important;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "# Model Deployment\n",
    "\n",
    "*Model formats, batch vs realtime scoring, and deployment pipelines*\n",
    "\n",
    "---\n",
    "\n",
    "Ethan Swan, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Topic: Deployment\n",
    "\n",
    "Once a model has been selected, we usually need to do something with it.\n",
    "\n",
    "*e.g.* We want to predict what products we should suggest to an online user. \n",
    "\n",
    "Setting up a model to run automatically on new data is called **deployment**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. About Me\n",
    "2. Exporting a Model\n",
    "3. Batch, Realtime, & On-demand Scoring\n",
    "4. Deployment Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- Senior Backend Developer at [ReviewTrackers](https://www.reviewtrackers.com/)\n",
    "    - Startup, ~100 employees\n",
    "    - SaaS platform for online reputation management\n",
    "- Analytics & ML Engineering Team\n",
    "- NLP Microservice (Python), Main API Layer (Go)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Started in February 2022\n",
    "- Wanted to see a techy startup from the inside\n",
    "    - especially engineering practices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- Previously: [84.51˚](https://www.8451.com/)\n",
    "    - Marketing Analytics Branch of Kroger\n",
    "    - Lead Data Scientist - Internal Tools & Infrastructure\n",
    "- Education: University of Notre Dame\n",
    "    - B.S. in Computer Science\n",
    "    - M.B.A."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- 8451:\n",
    "    - Did some measurement work\n",
    "    - quickly transitioned to functional support\n",
    "    - taught classses and helped with tech strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Model Deployment?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Do We Deploy Models?\n",
    "\n",
    "- A model is ultimately a function that maps inputs (*features*) to outputs (*targets*).\n",
    "    - Usually Python or R code\n",
    "- How can we use that function in a real-world application?\n",
    "    - How do we get new data into the model?\n",
    "    - What happens if we shut down the session? Is the model gone forever?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the Model\n",
    "\n",
    "1. Export (save) the model in a reusable place and format.\n",
    "2. Build batch, streaming, or on-demand scoring system that loads the model.\n",
    "3. Run the scoring system and feed it new data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models: Formats\n",
    "\n",
    "- Pickle\n",
    "    - Special, non--human-readable binary format\n",
    "    - Can save any Python object\n",
    "    - Some compatibility issues\n",
    "- Raw weights/parameters\n",
    "    - Just a bunch of numbers in a file\n",
    "    - More common for TensorFlow, PyTorch, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Pickle and similar libraries are easier and **more flexible**\n",
    "    - but compatibility concerns\n",
    "- raw model weights are **more portable**\n",
    "    - but not necessarily easy to reload\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exporting Models: Locations\n",
    "\n",
    "\n",
    "- Local filesystem\n",
    "    - Only if the model is going to be deployed locally\n",
    "- Cloud storage\n",
    "    - S3, GCS, Azure Blob Storage, etc.\n",
    "    - Works for almost any deployment location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deployment Approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch, Streaming, and On-demand Scoring\n",
    "- **Batch**\n",
    "    - Run the model in advance and save the output\n",
    "    - Think: Spotify Discover Weekly\n",
    "- **Realtime**\n",
    "    - Run the model on new data as it comes in\n",
    "    - Think: GitHub Copilot recommmends code as you type\n",
    "    - **Streaming**\n",
    "        - Queue up data and run the model \n",
    "        - Think: New \n",
    "    - **True On-demand**\n",
    "        - Run the model only when the prediction is needed via an API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Scoring\n",
    "\n",
    "Architecture\n",
    "- Airflow or cloud-based scheduler to kick off the model\n",
    "- Chain parts of the job (tasks) together\n",
    "- Save output to a persistent location: database or cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Scoring\n",
    "\n",
    "Pros\n",
    "- Predictable workload (always one run per hour/day/week)\n",
    "- Relatively easy to set up\n",
    "\n",
    "Cons\n",
    "- Predictions are stale until the next run\n",
    "- Reruns happen even if nothing has changed -- wasting resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Realtime Scoring\n",
    "\n",
    "Two main kinds...\n",
    "\n",
    "- **Streaming** – Trigger the model on new \"events\"\n",
    "- **True On-demand** – Access the model via an API when new predictions are needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch: Streaming\n",
    "\n",
    "Kick off a model run when a certain event occurs\n",
    "\n",
    "This is technically \"batch\" scoring, but it's realtime-ish\n",
    "\n",
    "- e.g. when a new customer data is uploaded, regenerate product recommendations\n",
    "\n",
    "\n",
    "Architecture\n",
    "- A \"publisher\" sends a message to a \"subscriber\" when an event occurs\n",
    "- Messages are \"queued\" up until the subscriber is ready to process them\n",
    "    - Thus not truly realtime\n",
    "- Platform: Kafka, RabbitMQ, cloud-based pub/sub, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch: Streaming\n",
    "\n",
    "Pros\n",
    "- Queues help manage spikes in workload\n",
    "- Consumers (scorers) can run in parallel\n",
    "    - Enables easy horizontal scaling\n",
    "- Queues add additional resilience\n",
    "    - A consumer crash doesn't result in lost data\n",
    "\n",
    "Cons\n",
    "- Requires an additional system (the message queue service)\n",
    "- Data flow through queues is difficult to trace and reason about\n",
    "- Large workloads can cause long backups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch: Manual\n",
    "\n",
    "This is exactly what you think: log in and kick off the scoring yourself.\n",
    "\n",
    "Pros\n",
    "- No setup\n",
    "\n",
    "Cons\n",
    "- Fragile, error-prone, and time-consuming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
